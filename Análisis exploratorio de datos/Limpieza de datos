#-----------------------------------------EDA--------------------------------------------------------------------
str(datos)
colnames(datos)
head(datos)
attach(datos)
nlevels(Tratamiento)
colnames(datos)
view(datos)


############################# Limpiar nombres de columnas#############################
datos <- clean_names(datos)
# Ver cómo quedaron
names(datos)
view(datos)

############################# Convertir variables ###############################
# Variables de respuesta a numérico
numericas <- c("humedad_pocentaje", "peso_seco_g", "peso_humedo_g", 
               "altura_tallo11", "diametro_tallo11", "altura_tallo12", 
               "diametro_tallo12", "altura_tallo13", "diametro_tallo13", 
               "prom_t_altura_tallo", "prom_t_diametro_tallo", 
               "total_promedio_de_diametro_prom_t", 
               "total_promedio_de_alt_prom_t", "peso_tallo_kg", 
               "peso_raiz_kg")

for(col in numericas){
  datos[[col]] <- as.numeric(as.vector(datos[[col]]))
}

# Verificación rápida
sapply(datos[numericas], class)


# Conversion a factores
datos$volumen <- as.factor(datos$volumen)
datos$susp <- as.factor(datos$susp)
datos$momentos_aplicacion <- as.factor(datos$momentos_aplicacion)

# Verificación
sapply(datos[c("volumen", "susp", "momentos_aplicacion")], class)



#######################Identificar datos faltantes#################################
colSums(is.na(datos))
datos$peso_seco_g[is.na(datos$peso_seco_g)] <- mean(datos$peso_seco_g, na.rm = TRUE)
colSums(is.na(datos[numericas]))

# Vector con nombres de las columnas numéricas
numericas <- c("humedad_pocentaje", "peso_seco_g", "peso_humedo_g", 
               "altura_tallo11", "diametro_tallo11", "altura_tallo12", 
               "diametro_tallo12", "altura_tallo13", "diametro_tallo13", 
               "prom_t_altura_tallo", "prom_t_diametro_tallo", 
               "total_promedio_de_diametro_prom_t", 
               "total_promedio_de_alt_prom_t", "peso_tallo_kg", 
               "peso_raiz_kg")

set.seed(123)  # Para reproducibilidad


# Vector de columnas con pocos NAs
pocas_na <- c("humedad_pocentaje", "peso_seco_g", "peso_humedo_g",
              "altura_tallo13", "prom_t_altura_tallo", "prom_t_diametro_tallo",
              "peso_tallo_kg", "peso_raiz_kg", "diametro_tallo13", "total_promedio_de_alt_prom_t", 
              "total_promedio_de_diametro_prom_t")

for(col in pocas_na){
  datos[[col]][is.na(datos[[col]])] <- median(datos[[col]], na.rm = TRUE)
}

# IMPUTACIÓN CON MODELO RANDOM FOREST
library(randomForest)

# Filas completas para entrenar el modelo
train_rf <- datos[complete.cases(datos[c("diametro_tallo11", "altura_tallo11", 
                                         "altura_tallo12", "diametro_tallo12", 
                                         "peso_tallo_kg")]), ]

# Ajustar modelo Random Forest
set.seed(123)
rf_model <- randomForest(diametro_tallo11 ~ altura_tallo11 + altura_tallo12 + diametro_tallo12 + peso_tallo_kg,
                         data = train_rf, ntree = 500)

# Predecir los NAs
indices_na <- which(is.na(datos$diametro_tallo11))
datos$diametro_tallo11[indices_na] <- predict(rf_model, newdata = datos[indices_na, 
                                                                        c("altura_tallo11", "altura_tallo12", 
                                                                          "diametro_tallo12", "peso_tallo_kg")])


####################### Revisar datos duplicados #############################################

# Verificar duplicados
sum(duplicated(datos))

# Eliminar duplicados si existen
datos <- datos[!duplicated(datos), ]

sum(duplicated(datos_limpios))  # cuántas filas duplicadas
datos_limpios <- datos_limpios[!duplicated(datos_limpios), ]  # eliminarlas

################# Revisar datos atipicos ######################################################

# Función para detectar outliers de una columna
detectar_outliers <- function(x){
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  which(x < (Q1 - 1.5*IQR) | x > (Q3 + 1.5*IQR))
}
outliers_lista <- list()

for(col in numericas){
  outliers <- detectar_outliers(datos[[col]])
  if(length(outliers) > 0){
    outliers_lista[[col]] <- outliers
  }
}

outliers_lista
filas_outliers <- unique(unlist(outliers_lista))

datos_limpios <- datos[-filas_outliers, ] #Eliminar outliers y crear nueva data limpia

cat("Número de filas eliminadas por contener outliers:", length(filas_outliers), "\n")
cat("Filas eliminadas:", filas_outliers, "\n")
outliers <- lapply(datos[numericas], detectar_outliers); outliers

outliers_valores <- lapply(names(outliers_lista), function(col){
  datos[outliers_lista[[col]], col]
})
names(outliers_valores) <- names(outliers_lista)
outliers_valores #Para ver exactamente los valores atípicos 




# Estadísticas ZCore (>3 o <3 es extremo)
z_scores <- scale(datos[numericas])
which(abs(z_scores) > 3, arr.ind = TRUE)  # filas y columnas con outliers

# Boxplots
boxplot(datos$diametro_tallo11, main="Diametro Tallo 11", col="lightblue")
boxplot(datos[numericas], las=2, col="lightgreen", main="Boxplots de todas las variables numéricas")

# Gráfico de Violin
datos_largo <- pivot_longer(datos[numericas], cols = everything(), names_to = "variable", values_to = "valor")

ggplot(datos_largo, aes(x=variable, y=valor)) +
  geom_violin(fill="lightblue") +
  geom_jitter(width=0.1, alpha=0.3, color="red") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Distribución de variables numéricas con posibles outliers")

#Gráfico de densidad y de pares
pairs(datos[numericas], pch=19, col=rgb(0,0,1,0.5))

ggplot(datos_largo, aes(x = valor)) +
  geom_density(fill = "lightblue", alpha = 0.5) +
  facet_wrap(~variable, scales = "free") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Distribución de variables numéricas")

######################## Revisión de consistencia de valores #######################################

lapply(datos_limpios[, c("susp", "volumen", "momentos_aplicacion")], levels) #para factores
summary(datos_limpios[numericas]) #para numéricas, revisar mínimos y máximos













####################################################################################################################################
#ANÁLISIS DE LA BASE DE DATOS
setwd("C:/Users/LUIS/Desktop/USAC 2/9no. semestre/MIAPA/Datos de práctica") #Abrir directorio de trabajo

#Carga de liberías-----------------------------------------------------------------------
install.packages("mice")
install.packages("corrplot")
install.packages("GGally")
install.packages("fBasics")
library(fBasics)
library(GGally)
library(mice) #para realizar imputación múltiple
library(corrplot)
library(readxl)
library(doebioresearch)
library(performance)
library(dplyr)
library(ScottKnott)
library(agricolae)
library(car)
library(broom)
library(data.table)        
library(emmeans)
library(ggplot2)
library(tidyverse)
library(lattice)
library(nlme)
library(lme4)
library(lmerTest)
library(multcomp)
library(rstatix)
library(ggpubr)
library(see)




#Importación de datos---------------------------------------------------
#Importar archivo Excel:
datos<- read_excel("Aserradero.xlsx", sheet = 1)
view(datos)
attach(datos)
  
#Convertir columna de interés (que sea categórica) a factor para realizar ANDEVA:
  
view(datos)
datos$rendimiento<- as.factor(datos$rendimiento) #Para convertir la columna a factor en caso de que la variable sea cualitativa
                                                    #Sustituir rendimiento por la variable categórica 
  
  
  
  
#Importar archivo CSV (delimitado por comas):
data <- read.csv("ruta/al/archivo.csv", stringsAsFactors = FALSE) #stringsAsFactors es para indicar si se desea que las columnas con datos categóricos representan o no factores a ser usados en el ANDEVA (Factor con tratamientos). Si se coloca False se consideran como nombres nada más.
                                                                      #Columna con nombres o caracteres normales: FALSE
                                                                      #col. con tratamientos (Factor para ANDEVA): TRUE
#Importar archivo txt:
data <- read.table("ruta/al/archivo.txt", header = TRUE, sep = "\t", stringsAsFactors = FALSE) #StringsAsFactors es aplicable para read.CSV y read.table ya q se derivan de la misma fx. No aplica para read.excel
                                                                                #header=TRUE indica que la primera fila serán los nombres de las columnas
                                                                                #sep="\t" es la tabulación por la que se separaran las columnas
  
  
  
  
  
  
  
#Exploración inicial de datos-----------------------------------------------
str(datos) #Estructura de los datos: indica nombre de las columnas, el número de datos, número de filas y columnas
dim(datos) #No.filas y columnas del dataframe
summary(datos) #Resumen de medidas de tendencia central y dispersión (media, mediana, cuartiles, max, min)
head(datos) #Primeros y ultimos 6 datos 
tail(datos)  

  
#Manejo de datos faltantes--------------------------------------------------------
            
              #Tipos de datos faltantes:
                  # 1) DF completamente al azar: puede imputarse con media, mediana, moda (datos categ)
                  # 2) DF al azar: imputar con PMM o con mediana o con distrib normal
                  # 3) DF no aleatorios: imputar con PMM o con KNN


#Verificar si hay datos faltantes en el dataframe:

anyNA(datos) #para verificar si hay datos faltantes en la base de datos
is.na(datos) #para saber en que filas y columnas se localizan los datos faltantes en el dataframe
colSums((is.na(datos))) # Contar los NA por columna:
faltantes <- colSums(is.na(datos))
datos_faltantes <- data.frame(Variable = names(faltantes), Faltantes = faltantes) # Convertir a data frame

# Graficar
ggplot(datos_faltantes, aes(x = Variable, y = Faltantes)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Datos faltantes por variable") +
  theme_minimal()



  
# Manejo de datos faltantes (NA):
    
# 1) Eliminar NA:
na.omit(datos) 
datos_limpios<- na.omit(datos)
view(datos_limpios)
    
    
    
    
# 2) Imputación simple (a través de la media):
    
rendimiento_media<- datos %>%
mutate("rendimiento" = ifelse(is.na(rendimiento), mean(rendimiento, na.rm = TRUE), rendimiento))
    
    #Estructura básica de ifelse: ifelse(condición, valor_si_verdadero, valor_si_falso)
    #Indica que si hay datos faltantes, entonces rellenar con la media, si no; dejar el valor original
    #Al usar el operador pipe (%>%) no hay necesidad de colocar datos%rendimiento para acceder a la columna rendimiento
    # pipe sirve para encadenar "datos" a la fx mutate. Mutate adiciona o modifica las columnas con la instrucción de "ifelse".                                                      

rendimiento_media #Correr para verificar si se sustituyeron los datos faltantes

#Gráfico de dens. prob (datos originales vs imputados):
plot(density(datos$rendimiento, na.rm = TRUE), col="red", xlab = "Rendimiento", ylab = "Densidad de prob.", main = "Imputación (media)")
lines(density(rendimiento_media$rendimiento), col = "blue")
    
    legend("topright",                      
           legend = c("Original", "Imputado (media)"),
           col = c("red", "blue"),          
           lty = 1,                         
           cex = 0.6)       
    
    
    
    
# 3) Imputación simple (a través de la mediana: es una forma más robusta de imputar cuando no hay claridad sobre la distribución de los datos, anula el efecto de outliers)
    
rendimiento_mediana<- datos %>%
  mutate("rendimiento" = ifelse(is.na(rendimiento), median(rendimiento, na.rm = TRUE), rendimiento))
    
rendimiento_mediana #correr para verificar si la imputación fue exitosa

plot(density(datos$rendimiento, na.rm = TRUE), col="red", xlab = "Rendimiento", ylab = "Densidad de prob.", main = "Imputación (mediana)")
lines(density(rendimiento_mediana$rendimiento), col = "blue")
    
    legend("topright",                      
           legend = c("Original", "Imputado (media)"),
           col = c("red", "blue"),          
           lty = 2,                         
           cex = 0.4)
  

      

# 4) Imputación mediante la distribución normal (solo si los datos son biométricos):
media<- mean(datos$rendimiento, na.rm = TRUE)
DE<- sd(datos$rendimiento, na.rm = TRUE)
conteo_NA<- sum(is.na(datos$rendimiento))
datos_imputados<- rnorm(conteo_NA, mean = media, sd = DE)
print(datos_imputados)
datos_nuevos<- datos
datos_nuevos$rendimiento[is.na(datos_nuevos$rendimiento)]<- datos_imputados #Se introdujeron los datos nuevos o imputados a la columna de interés


    # Comparación gráfica de las distribuciones (datos originales vs normal):
ggplot(datos, aes(x = rendimiento)) +
  geom_density(color = "blue", linewidth = 1.2, fill = "blue", alpha = 0.3) +
  stat_function(fun = dnorm, args = list(mean = media, sd = DE), color = "red", linetype = "dashed", linewidth = 1.2) +
  labs(title = "Comparación entre densidad empírica y normal", x = "Rendimiento", y = "Densidad de probabilidad") +
  theme_minimal() + xlim(2000, 10000)
  
    # Curva de dens. prob de datos originales e imputados:
plot(density(datos$rendimiento, na.rm = T), col = "red", xlab = "Rendimiento", ylab = "Densidad de prob.", main = "Imputación (D. normal)")
lines(density(datos_nuevos$rendimiento), col = "blue")

legend("topright",                      
       legend = c("Original", "Imputado (PMM)"),
       col = c("red", "blue"),          
       lty = 1,                         
       cex = 0.45)



# 5) Imputación múltiple por coincidencia de media predictiva (PMM):

    #Verificar si hay relación lineal entre las variables mediante análisis de correlación:
head(datos)
cor(`peso total`, rendimiento, use = "complete.obs")    
modelo<- lm(rendimiento ~ `peso total`)
summary(modelo)
plot(`peso total`, rendimiento)
abline(modelo, lwd=2, col = "red")


        # PMM cuando se toma solo un dataset
imputado<- mice(datos, method = "pmm", seed = 123, m = 5) # m es el número de datasets, seed indica q los resultados serán reproducibles
summary(imputado) #para ver en qué columna se aplicó la imputación
imputado$imp$rendimiento #para visualizar cuales fueron los valores imputados
datos_imputados<- complete(imputado, 1) #para seleccionar uno de los 5 conjuntos de datos creados
print(datos_imputados)

      # Diagrama de dispersión: datos imputados vs originales

plot(datos$`peso total`, datos$rendimiento, col = "red", pch = 16,
     xlab = "Peso total", ylab = "Rendimiento", main = "Datos originales vs imputados")

points(datos_imputados$`peso total`, datos_imputados$rendimiento, col = "blue", pch = 1)
legend("bottomright", legend = c("Original", "Imputado"), col = c("red", "blue"), pch = c(16,1))


      # Curva de densidad de probabilidad: datos imputados vs originales

plot(density(datos$rendimiento, na.rm = TRUE), col="red", xlab = "Rendimiento", ylab = "Densidad de prob.", main = "Imputación (PMM)")
lines(density(datos_imputados$rendimiento), col = "blue")

legend("topright",                      
       legend = c("Original", "Imputado (PMM)"),
       col = c("red", "blue"),          
       lty = 1,                         
       cex = 0.6)

#Limpieza de la base de datos-----------------------------------------------------------------------
  #Detección y corrección de errores en los datos
    #Visualizar valores atípicos
boxplot(datos$rendimiento)
outliers<- boxplot(datos$rendimiento, plot = FALSE)$out #para saber qué dato es el atípico
Q1<- quantile(datos$rendimiento, 0.25, na.rm = TRUE)
Q3<- quantile(datos$rendimiento, 0.75, na.rm = T )
IQR<- Q3-Q1
limite_superior<- Q3+1.5*IQR
limite_inferior<- Q1-1.5*IQR
which(datos$rendimiento %in% outliers) #Indica la fila en la que se encuentra el valor atípico según la columna analizada

datos$outlier<- ifelse(datos$rendimiento < limite_inferior| datos$rendimiento>limite_superior, "si","no") #crea una columna para indicar qué datos son atípicos y cuáles no

ggplot(datos, aes(x = "", y = rendimiento)) +
  geom_boxplot(outlier.colour = "red", outlier.size = 3) +
  labs(title = "Boxplot de rendimiento con outliers en rojo", y = "Rendimiento") +
  theme_minimal()

    #Identificación de valores extremos
summary(datos$rendimiento)
    #Identificar valores únicos de la variable
unique(datos$rendimiento)
  
  #Normalización de datos al mismo formato
datos$rendimiento<- as.factor(datos$rendimiento) #para convertir a factor la columna de los tratamientos
datos$rendimiento<- as.numeric(datos$rendimiento) #convertir la variable respuesta a datos

  #Corrección de nombres de los tratamientos (todos en minúscula)
datos$rendimiento<- tolower(trimws(datos$rendimiento))

  #Eliminar datos duplicados
datos<- datos[!duplicated(datos), ]

  # Matriz de correlación para identificación de correlaciones sospechosas entre variables numéricas (indicativo de errores)
matriz_cor<- cor(datos[, sapply(datos, is.numeric)], use = "complete.obs")
  #Mapa de calor (Pearson):
corrplot(matriz_cor, method = "color", type = "upper", 
         tl.col = "black", tl.srt = 45, addCoef.col = "black")
  #Matriz de correlación
GGally::ggpairs(datos[, sapply(datos, is.numeric)])

  #Limpiar errores de digitación:
# Eliminar espacios en blanco antes y después
datos$rendimiento <- trimws(datos$rendimiento) #Aplicar sobre factores con tratamientos categóricos

#Detectar filas duplicadas

duplicados <- datos[duplicated(datos), ]
print(duplicados)
datos <- datos[!duplicated(datos), ] #Eliminar duplicados

#Corrección de errores de unidad o escala (ej: datos que no corresponan a 100 kg/ha, que soprepasen ese valor)

  # Visualizar valores extremos
boxplot(datos$rendimiento, main = "Boxplot para detectar posibles errores de unidad")

  # Ver valores mayores a cierto umbral
datos[datos$rendimiento > 5000, ]  # Ajusta el umbral según la variable

# Unificar nombres (todo mayúscula y sin espacios)
datos$rendimiento <- toupper(trimws(datos$rendimiento)) #Para columna de los tratamientos




